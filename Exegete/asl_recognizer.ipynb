{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19c3e31-d775-4496-a7e3-839702cb399a",
   "metadata": {},
   "source": [
    "# Final Project: ASL Recognition\n",
    "### Professor: Weizhe Li\n",
    "### Student: Levan Sulimanov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375465a-7c1b-4726-a178-1eb77ce06435",
   "metadata": {},
   "source": [
    "# Getting the model defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d13a6fb0-3843-4db5-8020-c06c147581cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model:\n",
    "###################################### CREDITS ######################################\n",
    "# https://learnopencv.com/human-action-recognition-using-detectron2-and-lstm/\n",
    "#####################################################################################\n",
    "\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from numpy import genfromtxt\n",
    "\n",
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, data_arr):\n",
    "        self.data_arr = data_arr\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_arr)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_path = self.data_arr[idx][0]\n",
    "        y = self.data_arr[idx][1]\n",
    "        \n",
    "        X = torch.from_numpy(genfromtxt(X_path, delimiter=',')).float()\n",
    "        \n",
    "        # print(\"X:\", X.shape)\n",
    "        # print(\"y:\", y)\n",
    "        \n",
    "        return X, y\n",
    "\n",
    "\n",
    "class PoseDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_root, batch_size=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_root = data_root\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "    def load_X_and_Y(self, data_path, train_mode=\"train\"):\n",
    "        \n",
    "        data_x_y_arr = []\n",
    "        \n",
    "        read_from_dir = os.path.join(data_path, train_mode)\n",
    "        \n",
    "        # go over each class folder, get subsamples per class, and return tuples [sample_path, class_number]\n",
    "        for class_folder in os.listdir(read_from_dir):\n",
    "            class_abs_path = os.path.join(read_from_dir, class_folder)\n",
    "            for sample in os.listdir(class_abs_path):\n",
    "                sample_path = os.path.join(class_abs_path, sample)\n",
    "                class_num = class_to_label_num[class_folder]\n",
    "                data_x_y_arr.append([sample_path, class_num])\n",
    "\n",
    "        # print(f\"Collected full list for {train_mode}:\\n{data_x_y_arr}\")\n",
    "        return data_x_y_arr\n",
    "            \n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        train_data_arr = self.load_X_and_Y(self.data_root, train_mode=\"train\")\n",
    "        eval_data_arr = self.load_X_and_Y(self.data_root, train_mode=\"val\")\n",
    "        # test_data_arr = self.load_X_and_Y(self.data_root, train_mode=\"test\")\n",
    "        self.train_dataset = PoseDataset(train_data_arr)\n",
    "        self.val_dataset = PoseDataset(eval_data_arr)\n",
    "        # self.test_dataset = PoseDataset(test_data_arr)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # train loader\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # validation loader\n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "        return val_loader\n",
    "    '''\n",
    "    def test_dataloader(self):\n",
    "        # validation loader\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=8\n",
    "        )\n",
    "        return test_loader\n",
    "    '''\n",
    "\n",
    "# We have 6 output action classes.\n",
    "TOT_ACTION_CLASSES = 9 #6\n",
    "\n",
    "#lstm classifier definition\n",
    "class ActionClassificationLSTM(pl.LightningModule):\n",
    "    # initialise method\n",
    "    def __init__(self, input_features, hidden_dim, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        # save hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(input_features, hidden_dim, batch_first=True)\n",
    "        # The linear layer that maps from hidden state space to classes\n",
    "        self.linear = nn.Linear(hidden_dim, TOT_ACTION_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # invoke lstm layer\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        # invoke linear layer\n",
    "        return self.linear(ht[-1])\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # get data and labels from batch\n",
    "        x, y = batch\n",
    "        # reduce dimension\n",
    "        y = torch.squeeze(y)\n",
    "        # convert to long\n",
    "        y = y.long()\n",
    "        # get prediction\n",
    "        y_pred = self(x)\n",
    "        # calculate loss\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        # get probability score using softmax\n",
    "        prob = F.softmax(y_pred, dim=1)\n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]\n",
    "        # calculate accuracy\n",
    "        acc = torchmetrics.functional.accuracy(pred, y, task=\"multiclass\", num_classes=TOT_ACTION_CLASSES)\n",
    "        dic = {\n",
    "            'batch_train_loss': loss,\n",
    "            'batch_train_acc': acc\n",
    "        }\n",
    "        # log the metrics for pytorch lightning progress bar or any other operations\n",
    "        self.log('batch_train_loss', loss, prog_bar=True)\n",
    "        self.log('batch_train_acc', acc, prog_bar=True)\n",
    "        #return loss and dict\n",
    "        return {'loss': loss, 'result': dic}\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        # calculate average training loss end of the epoch\n",
    "        avg_train_loss = torch.tensor([x['result']['batch_train_loss'] for x in training_step_outputs]).mean()\n",
    "        # calculate average training accuracy end of the epoch\n",
    "        avg_train_acc = torch.tensor([x['result']['batch_train_acc'] for x in training_step_outputs]).mean()\n",
    "        # log the metrics for pytorch lightning progress bar and any further processing\n",
    "        self.log('train_loss', avg_train_loss, prog_bar=True)\n",
    "        self.log('train_acc', avg_train_acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # get data and labels from batch\n",
    "        x, y = batch\n",
    "        # reduce dimension\n",
    "        y = torch.squeeze(y)\n",
    "        # convert to long\n",
    "        y = y.long()\n",
    "        # get prediction\n",
    "        y_pred = self(x)\n",
    "        # calculate loss\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        # get probability score using softmax\n",
    "        prob = F.softmax(y_pred, dim=1)\n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]\n",
    "        # calculate accuracy\n",
    "        acc = torchmetrics.functional.accuracy(pred, y, task=\"multiclass\", num_classes=TOT_ACTION_CLASSES)\n",
    "        dic = {\n",
    "            'batch_val_loss': loss,\n",
    "            'batch_val_acc': acc\n",
    "        }\n",
    "        # log the metrics for pytorch lightning progress bar and any further processing\n",
    "        self.log('batch_val_loss', loss, prog_bar=True)\n",
    "        self.log('batch_val_acc', acc, prog_bar=True)\n",
    "        #return dict\n",
    "        return dic\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        # calculate average validation loss end of the epoch\n",
    "        avg_val_loss = torch.tensor([x['batch_val_loss']\n",
    "                                     for x in validation_step_outputs]).mean()\n",
    "        # calculate average validation accuracy end of the epoch\n",
    "        avg_val_acc = torch.tensor([x['batch_val_acc']\n",
    "                                    for x in validation_step_outputs]).mean()\n",
    "        # log the metrics for pytorch lightning progress bar and any further processing\n",
    "        self.log('val_loss', avg_val_loss, prog_bar=True)\n",
    "        self.log('val_acc', avg_val_acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # adam optimiser\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        # learning rate reducer scheduler\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, min_lr=1e-15, verbose=True)\n",
    "        # scheduler reduces learning rate based on the value of val_loss metric\n",
    "        return {\"optimizer\": optimizer,\n",
    "                \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"epoch\", \"frequency\": 1, \"monitor\": \"val_loss\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328774a7-b7e7-4def-8391-b94de8da8b35",
   "metadata": {},
   "source": [
    "# Main Real-Time Core Part of Exegete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df394fd7-e961-431b-8565-01d8a6dc3c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "                                                 \n",
      " ________________________________________________\n",
      "/_____/_____/_____/_____/_____/_____/_____/_____/\n",
      "                                                 \n",
      "                                                 \n",
      "\n",
      "    ______                     __     \n",
      "   / ____/  _____  ____ ____  / /____ \n",
      "  / __/ | |/_/ _ \\/ __ `/ _ \\/ __/ _ \\\n",
      " / /____>  </  __/ /_/ /  __/ /_/  __/\n",
      "/_____/_/|_|\\___/\\__, /\\___/\\__/\\___/ \n",
      "                /____/                \n",
      "\n",
      "                                                 \n",
      "                                                 \n",
      " ________________________________________________\n",
      "/_____/_____/_____/_____/_____/_____/_____/_____/\n",
      "                                                 \n",
      "                                                 \n",
      "\n",
      "mp.solutions: <module 'mediapipe.python.solutions' from 'C:\\\\Users\\\\lrspr\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\mediapipe\\\\python\\\\solutions\\\\__init__.py'>\n",
      "#######################\n",
      "Connecting to Device...\n",
      "#######################\n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Selected Device: 0\n",
      "Selected Model Path: C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\models\\9_class_model_base_lstm.ckpt\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "\n",
      "<<<           ASL is called        >>>\n",
      "<<<      Starting Recognition!!!   >>>\n",
      "\n",
      "\n",
      "\n",
      "<<<      Closing ASL Exagete      >>>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Library imports:\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "# from src.lstm import ActionClassificationLSTM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import winsound\n",
    "from threading import Thread\n",
    "from itertools import groupby\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from pyfiglet import Figlet\n",
    "import traceback\n",
    "import imutils\n",
    "\n",
    "\n",
    "# Define Fixed Video Size\n",
    "MAIN_WIDTH, MAIN_HEIGHT = 640, 480\n",
    "\n",
    "\n",
    "# Define output labels (class numbers association from LSTM model)\n",
    "LABELS = {0: \"hello\", 1: \"my\", 2: \"world\", 3: \"me\", 4: \"every\", 5: \"moment\", 6: \"is\", 7: \"new\", 8: \"beginning\"}\n",
    "\n",
    "# add directory, if it does not exists\n",
    "def mkdir_if_none(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "\n",
    "# Pad the frame to specified size\n",
    "def padding(img, expected_size):\n",
    "    desired_size = expected_size\n",
    "    delta_width = desired_size - img.size[0]\n",
    "    delta_height = desired_size - img.size[1]\n",
    "    # get centers\n",
    "    pad_width = delta_width // 2\n",
    "    pad_height = delta_height // 2\n",
    "    # add padding:\n",
    "    padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
    "    return ImageOps.expand(img, padding)\n",
    "\n",
    "# resize the frame to specified size\n",
    "def resize_with_padding(img, expected_size):\n",
    "    delta_width = expected_size[0] - img.size[0]\n",
    "    delta_height = expected_size[1] - img.size[1]\n",
    "    pad_width = delta_width // 2\n",
    "    pad_height = delta_height // 2\n",
    "    padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
    "    return ImageOps.expand(img, padding)\n",
    "\n",
    "# total number of joints detected (hands, elbows, and shoulders):\n",
    "MAX_NUM_OF_XY_KEYPOINTS_LIST = 46\n",
    "\n",
    "# if something errored out, return pre-defined array:\n",
    "BACKUP_ARRAY = np.array([2.0, 2.0] * MAX_NUM_OF_XY_KEYPOINTS_LIST)\n",
    "\n",
    "    \n",
    "# just get keypoints from frame:\n",
    "def get_keypoints_from_frame(frame, results, mp_holistic, switch_turned_on, hand_keypoint_default=21, verbose=True):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        # if it came from OpenCV, we need to switch channel order:\n",
    "        frame = imutils.resize(frame, width=MAIN_WIDTH)\n",
    "        pil = Image.fromarray(frame)\n",
    "        frame = cv2.cvtColor(np.array(resize_with_padding(pil, (MAIN_WIDTH, MAIN_HEIGHT))), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # make detections:\n",
    "        \n",
    "        frame_coordinates = []\n",
    "            \n",
    "        ################################################################################\n",
    "        # get coordinates:\n",
    "        # hand points:\n",
    "        # RIGHT:\n",
    "        if results.right_hand_landmarks:\n",
    "            for r_h in results.right_hand_landmarks.landmark:\n",
    "                frame_coordinates.append(r_h.x)\n",
    "                frame_coordinates.append(r_h.y)\n",
    "        else:\n",
    "            for r_h in range(21):\n",
    "                frame_coordinates.append(2.0)\n",
    "                frame_coordinates.append(2.0)\n",
    "\n",
    "        # LEFT:\n",
    "        if results.left_hand_landmarks:\n",
    "            for l_h in results.left_hand_landmarks.landmark:\n",
    "                frame_coordinates.append(l_h.x)\n",
    "                frame_coordinates.append(l_h.y)\n",
    "        else:\n",
    "            for l_h in range(21):\n",
    "                frame_coordinates.append(2.0)\n",
    "                frame_coordinates.append(2.0)\n",
    "\n",
    "        # SHOULDERS AND ELBOWS:\n",
    "        if results.pose_landmarks:\n",
    "            # top torse keypoints:\n",
    "            r_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW]\n",
    "            frame_coordinates.append(r_elbow.x)\n",
    "            frame_coordinates.append(r_elbow.y)\n",
    "\n",
    "            l_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_ELBOW]\n",
    "            frame_coordinates.append(l_elbow.x)\n",
    "            frame_coordinates.append(l_elbow.y)\n",
    "\n",
    "            r_shoulder = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER]\n",
    "            frame_coordinates.append(r_shoulder.x)\n",
    "            frame_coordinates.append(r_shoulder.y)\n",
    "\n",
    "            l_shoulder = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_SHOULDER]\n",
    "            frame_coordinates.append(l_shoulder.x)\n",
    "            frame_coordinates.append(l_shoulder.y)\n",
    "        else:\n",
    "            frame_coordinates.append(2.0)  # fake x axis for r_elbow\n",
    "            frame_coordinates.append(2.0)  # fake y axis for r_elbow\n",
    "            \n",
    "            frame_coordinates.append(2.0)  # fake x axis for l_elbow\n",
    "            frame_coordinates.append(2.0)  # fake y axis for l_elbow\n",
    "            \n",
    "            frame_coordinates.append(2.0)  # fake x axis for r_shoulder\n",
    "            frame_coordinates.append(2.0)  # fake y axis for r_shoulder\n",
    "            \n",
    "            frame_coordinates.append(2.0)  # fake x axis for l_shoulder\n",
    "            frame_coordinates.append(2.0)  # fake y axis for l_shoulder\n",
    "\n",
    "        if verbose:\n",
    "            for coord_idx in range(0, len(frame_coordinates), 2):\n",
    "                lm = frame_coordinates[coord_idx], frame_coordinates[coord_idx+1]\n",
    "                cx, cy = int(lm[0]*frame.shape[1]), int(lm[1]*frame.shape[0])\n",
    "                if switch_turned_on:\n",
    "                    cv2.circle(frame, (cx, cy), 4, (0,255,0), cv2.FILLED)\n",
    "                else:\n",
    "                    cv2.circle(frame, (cx, cy), 4, (0,0,255), cv2.FILLED)\n",
    "        ################################################################################\n",
    "        return np.array(frame_coordinates), frame\n",
    "        # return frame_coordinates, frame\n",
    "    except:\n",
    "        print(\"<<<ERROR IN GETTING KEYPOINTS>>>\")\n",
    "        print(traceback.format_exc())\n",
    "        print(\"---\")\n",
    "        return BACKUP_ARRAY, frame\n",
    "    \n",
    "\n",
    "    \n",
    "def detect_sign(sign_model, buffer):\n",
    "    \n",
    "    try:\n",
    "        # otherwise, if we our buffer filled up, load it into the model\n",
    "        model_input = torch.Tensor(np.array(buffer, dtype=np.float32))\n",
    "        model_input = torch.unsqueeze(model_input, dim=0)\n",
    "        y_pred = sign_model(model_input)\n",
    "        prob = F.softmax(y_pred, dim=1)\n",
    "        pred_index = prob.data.max(dim=1)[1].item()\n",
    "        # pop oldest item out, to let new frame in\n",
    "        # Retreive predicted label and convert it to its associated string\n",
    "        label = LABELS[pred_index]\n",
    "        conf = prob.data[0][pred_index].item()\n",
    "        return label, conf\n",
    "    except:\n",
    "        return None, 0.0\n",
    "\n",
    "# Main ASL Exagete pipeline\n",
    "def process_images(model_weights_path, capture_system_path,\n",
    "                   fps=30, coord_buffer_len=30, thresh=.70, running_string_line_len=30, filtered=False):\n",
    "\n",
    "    try:\n",
    "        # setup our LSTM and load its weights\n",
    "        model_path_dir = model_weights_path\n",
    "\n",
    "        lstm_classifier = ActionClassificationLSTM.load_from_checkpoint(model_path_dir)\n",
    "        lstm_classifier.eval()\n",
    "\n",
    "\n",
    "        # NEWNEWNENWE#####\n",
    "        # setup MediaPipe:\n",
    "        mp_holistic = mp.solutions.holistic\n",
    "        print(f\"mp.solutions: {mp.solutions}\")\n",
    "        ##################\n",
    "\n",
    "        # Setup webcam to be default ID\n",
    "        device_capture = capture_system_path\n",
    "\n",
    "        use_CAP_DSHOW = True\n",
    "\n",
    "        # If Online Stream, convert it to video frames through the pafy libary\n",
    "        if (not (capture_system_path.isdigit())) and (\"http\" in capture_system_path):\n",
    "            print(\"Connecting to Video Stream...\")\n",
    "            url = capture_system_path\n",
    "            use_CAP_DSHOW = False\n",
    "            video = pafy.new(url)\n",
    "            best = video.getbest(preftype=\"mp4\")\n",
    "            device_capture = best.url\n",
    "            print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "            print(\"Selected Device:\", capture_system_path)\n",
    "\n",
    "        # If it's offline, point device capture to that offline video path\n",
    "        elif (not (capture_system_path.isdigit())) and (not (\"http\" in capture_system_path)):\n",
    "            device_capture = capture_system_path\n",
    "            use_CAP_DSHOW = False\n",
    "            print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "            print(\"Selected Device:\", device_capture)\n",
    "\n",
    "        # Otherwise, select provided Device's ID\n",
    "        else:\n",
    "            print(\"#######################\")\n",
    "            print(\"Connecting to Device...\")\n",
    "            print(\"#######################\\n\")\n",
    "            device_capture = int(capture_system_path)\n",
    "            use_CAP_DSHOW = True\n",
    "            print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "            print(\"Selected Device:\", device_capture)\n",
    "\n",
    "\n",
    "        print(\"Selected Model Path:\", model_path_dir)\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\\n\")\n",
    "\n",
    "        print(\"<<<           ASL is called        >>>\")\n",
    "        print(\"<<<      Starting Recognition!!!   >>>\\n\")\n",
    "\n",
    "\n",
    "        # Start getting frames from the chosen device / stream / video\n",
    "        if use_CAP_DSHOW:\n",
    "            cap = cv2.VideoCapture(device_capture, cv2.CAP_DSHOW)\n",
    "        else:\n",
    "            cap = cv2.VideoCapture(device_capture)\n",
    "        cap.set(cv2.CAP_PROP_FPS, fps)\n",
    "\n",
    "        # Add buffer - list of past 32 consecutive pose coordinates (from last 32 frames)\n",
    "        coord_buffer = []\n",
    "        coord_buffer_max = coord_buffer_len\n",
    "\n",
    "        recognized_string = \" \" * running_string_line_len\n",
    "        recognized_string_max = running_string_line_len\n",
    "\n",
    "        success = True\n",
    "        try:\n",
    "            _, frame = cap.read()\n",
    "            # WARNING <<< IF YOU WILL CHANGE SHAPE OF IMAGE (FOR SPEED), do it here too\n",
    "            height, width = frame.shape[0], frame.shape[1]\n",
    "        except:\n",
    "            print(\"Failed to read first frame\")\n",
    "            success = False\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "        \n",
    "        frame_count = 0\n",
    "        switch_turned_on = True\n",
    "        \n",
    "        if success:\n",
    "            with mp_holistic.Holistic(min_detection_confidence=0.5,\n",
    "                                      min_tracking_confidence=0.5,\n",
    "                                      static_image_mode=False) as holistic:\n",
    "                while cap.isOpened():\n",
    "                    \n",
    "                    if len(recognized_string) >= recognized_string_max:\n",
    "                        recognized_string = \"\"\n",
    "                    \n",
    "                    ret, frame = cap.read()\n",
    "\n",
    "                    # recolor feed:\n",
    "                    # image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                    # make detections:\n",
    "                    results = holistic.process(frame)\n",
    "\n",
    "                    main_coordinates = []     \n",
    "\n",
    "                    ################################################################################\n",
    "                    # get coordinates:\n",
    "                    curr_points_set, frame = get_keypoints_from_frame(frame, results, mp_holistic, switch_turned_on, hand_keypoint_default=21, verbose=True)\n",
    "\n",
    "                    if len(coord_buffer)<coord_buffer_max:\n",
    "                        coord_buffer.append(curr_points_set)\n",
    "                    else:\n",
    "                        label, conf = detect_sign(lstm_classifier, coord_buffer)\n",
    "                        if conf >= thresh:\n",
    "                            recognized_string += f\" {label}\"\n",
    "                            recognized_string = recognized_string[-recognized_string_max:]\n",
    "                        else:\n",
    "                            print(f\"{label} | {round(conf*100, 2)}\")\n",
    "                            label = None\n",
    "                        coord_buffer = []\n",
    "                        coord_buffer.append(curr_points_set)\n",
    "\n",
    "\n",
    "                    # overlay recognized string to frame:\n",
    "                    cv2.putText(frame, recognized_string.lstrip(),\n",
    "                                   (10, MAIN_HEIGHT-50), cv2.FONT_HERSHEY_COMPLEX, 0.9, (102, 255, 255), 4)\n",
    "                    ################################################################################\n",
    "                    if frame_count % fps == 0:\n",
    "                        if switch_turned_on:\n",
    "                            switch_turned_on = False\n",
    "                        else:\n",
    "                            switch_turned_on = True\n",
    "                    # if switch_turned_on:\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    cv2.imshow('Raw Webcam Feed', frame)\n",
    "                    frame_count+=1\n",
    "                    k = cv2.waitKey(1)\n",
    "                    if k == 27: # if Esc pressed\n",
    "                        break\n",
    "\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "            print(\"\\n\")\n",
    "            print(\"<<<      Closing ASL Exagete      >>>\\n\")\n",
    "    except:\n",
    "        print(\"<<<Something went wrong while running ASL Translation... Refer to the error description below.>>>\")\n",
    "        print(traceback.format_exc())\n",
    "        print(\"===========\\n\")\n",
    "\n",
    "def main():\n",
    "\n",
    "    # stylistic printout of ASL Exagete software\n",
    "    f = Figlet(font='slant')\n",
    "    print(f.renderText('--------'))\n",
    "    print(f.renderText('Exegete'))\n",
    "    print(f.renderText('--------'))\n",
    "    time.sleep(2.5)\n",
    "\n",
    "    # let user pass model_Weights name and video device (webcam ID, offline or online stream video)\n",
    "    parser = argparse.ArgumentParser(description=\"Initialize ASL Exagete's settings.\")\n",
    "    parser.add_argument('--model_dir', type=str, help='Model Weigths path from ./model dir.', default=os.path.join(os.getcwd(), \"models\"))\n",
    "    parser.add_argument('--model_name', type=str, help='Model Weigths path from ./model dir.', default=\"9_class_model_base_lstm.ckpt\")\n",
    "    parser.add_argument('--device_path', type=str, help='Either pass *device number* for WebCam, or YouTube *Link* for certain video stream (offline or online).', default=\"0\")\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    # set model weights path\n",
    "    # filter out extension, to add the correct model weights name\n",
    "    if args.model_name.endswith(\".ckpt\"):\n",
    "        pass\n",
    "    else:\n",
    "        args.model_name = f\"{args.model_name}.ckpt\"\n",
    "    model_name = os.path.join(args.model_dir, args.model_name)\n",
    "\n",
    "    # set camera ID or stream path:\n",
    "    capture_system_path = str(args.device_path)\n",
    "\n",
    "    # start our ASL Exagete software\n",
    "    process_images(model_name, capture_system_path, thresh=.85)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3880360-cec2-4833-a44b-da0dc0dc9c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
