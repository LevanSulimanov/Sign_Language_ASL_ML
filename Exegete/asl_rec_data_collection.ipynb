{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516a0749-84be-4c96-9529-2bdbabfc715b",
   "metadata": {},
   "source": [
    "# Final Project: ASL Recognition\n",
    "### Professor: Weizhe Li\n",
    "### Student: Levan Sulimanov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d9bea-1e34-4529-b002-9e6cf2cac4b6",
   "metadata": {},
   "source": [
    "# Data Collection Script:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7bcb01-9b8b-495c-af3d-92364d8f9750",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Install and Import Dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29dc0f3c-7034-4d90-952f-d93b1b8ba7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eea4e6a-494a-449a-9185-1ab53f670e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: opencv-python in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from opencv-python) (1.23.4)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: mediapipe in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from mediapipe) (22.10.26)\n",
      "Requirement already satisfied: numpy in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from mediapipe) (1.23.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from mediapipe) (4.6.0.66)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from mediapipe) (1.3.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from mediapipe) (3.6.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from matplotlib->mediapipe) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from matplotlib->mediapipe) (9.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from matplotlib->mediapipe) (4.38.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from matplotlib->mediapipe) (1.0.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lrspr\\anaconda3\\envs\\asl_rec\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install mediapipe\n",
    "!pip install pandas openpyxl\n",
    "!pip install Pillow\n",
    "!pip install imutils\n",
    "!pip install keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bcaff86-fd02-4fd7-b593-d99cea06f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import urllib.request  # for downloading videos\n",
    "import traceback\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import imutils\n",
    "import random\n",
    "import keyboard  # using module keyboard\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e10bce-e0cf-4a3b-b6f1-204e795cc6d6",
   "metadata": {},
   "source": [
    "# 1. Make Detections from Feed:\n",
    "1. Detect Hand Poses\n",
    "2. Detect Body Poses\n",
    "3. Collect only:\n",
    "    - Wrists, fingers\n",
    "    - Shoulders\n",
    "    - Elbows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a4b37a3-18eb-4f77-b5d0-83f720dc3b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test keypoints detection accuracy using Webcam and check if it works:\n",
    "def web_demo():\n",
    "    \n",
    "    # setup media pipe's holistic model for keypoints detections\n",
    "    # mp_drawing = mp.solutions.drawing_utils  # we are not using this one, since we are annotating detected keypoints using OpenCV drawing module\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    \n",
    "    print(f\"mp.solutions: {mp.solutions}\")\n",
    "\n",
    "    # setup webcam:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Check if the video is opened correctly\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "    # if success value will switch off after reading cap(), we will turn off OpenCV right away\n",
    "    success = True\n",
    "    try:\n",
    "        _, frame = cap.read()\n",
    "        # WARNING <<< IF YOU WILL CHANGE SHAPE OF IMAGE (FOR SPEED), do it here too\n",
    "        height, width = frame.shape[0], frame.shape[1]\n",
    "    except:\n",
    "        print(\"Failed to read first frame\")\n",
    "        success = False\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # if we are good at reading from video source, then let's run holistic model and input incoming frame into it to get keypoints \n",
    "    if success:\n",
    "        with mp_holistic.Holistic(min_detection_confidence=0.5,\n",
    "                                  min_tracking_confidence=0.5,\n",
    "                                  static_image_mode=False) as holistic:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # recolor feed:\n",
    "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # make detections:\n",
    "                # once detection are made, let's verify if hands, elbows, and shoulders were detected.\n",
    "                results = holistic.process(image)\n",
    "\n",
    "\n",
    "                main_coordinates = []\n",
    "\n",
    "                # Just for drwaing:\n",
    "                '''\n",
    "                # drawing Right Hand\n",
    "                mp_drawing.draw_landmarks(image,\n",
    "                                          results.right_hand_landmarks,\n",
    "                                          mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "                # drawing Left Hand\n",
    "                mp_drawing.draw_landmarks(image,\n",
    "                                          results.left_hand_landmarks,\n",
    "                                          mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "                # drawing shoulders and elbows\n",
    "                mp_drawing.draw_landmarks(image,\n",
    "                                          results.pose_landmarks,\n",
    "                                          mp_holistic.POSE_CONNECTIONS)\n",
    "                '''                \n",
    "\n",
    "                ################################################################################\n",
    "                # get coordinates:\n",
    "                # hand points:\n",
    "                if results.right_hand_landmarks:\n",
    "                    for r_h in results.right_hand_landmarks.landmark:\n",
    "                        main_coordinates.append((r_h.x, r_h.y))\n",
    "                # if not detected, fill in out of bounds coordinates -> think it will help to have consistent error model to know that we can have that.\n",
    "                else:\n",
    "                    main_coordinates.append((2.0,2.0))\n",
    "                \n",
    "                # left hand points:\n",
    "                if results.left_hand_landmarks:\n",
    "                    for l_h in results.left_hand_landmarks.landmark:\n",
    "                        main_coordinates.append((l_h.x, l_h.y))\n",
    "                # if not detected, fill in out of bounds coordinates -> think it will help to have consistent error model to know that we can have that.\n",
    "                else:\n",
    "                    main_coordinates.append((2.0, 2.0))\n",
    "                \n",
    "                # get elbows and shoulders from detected pose:\n",
    "                if results.pose_landmarks:\n",
    "                    # top torse keypoints:\n",
    "                    r_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW]\n",
    "                    main_coordinates.append((r_elbow.x, r_elbow.y))\n",
    "\n",
    "                    l_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_ELBOW]\n",
    "                    main_coordinates.append((l_elbow.x, l_elbow.y))\n",
    "\n",
    "                    r_shoulder = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER]\n",
    "                    main_coordinates.append((r_shoulder.x, r_shoulder.y))\n",
    "\n",
    "                    l_shoulder = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_SHOULDER]\n",
    "                    main_coordinates.append((l_shoulder.x, l_shoulder.y))\n",
    "                else:\n",
    "                    main_coordinates.append((2.0, 2.0))\n",
    "                    main_coordinates.append((2.0, 2.0))\n",
    "                    main_coordinates.append((2.0, 2.0))\n",
    "                    main_coordinates.append((2.0, 2.0))\n",
    "                \n",
    "                # draw them on top of the given frame:\n",
    "                for lm in main_coordinates:\n",
    "                    cx, cy = int(lm[0]*width), int(lm[1]*height)\n",
    "                    cv2.circle(image, (cx, cy), 3, (0, 0, 255), cv2.FILLED)\n",
    "                ################################################################################\n",
    "                # visualize the output:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92302820-2a8f-40a4-9744-350ea669c82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mp.solutions: <module 'mediapipe.python.solutions' from 'C:\\\\Users\\\\lrspr\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\mediapipe\\\\python\\\\solutions\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "web_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c1c8b9-d367-4d66-b9c7-62fa1df5bfef",
   "metadata": {},
   "source": [
    "# 2. Data Collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97c42134-4030-414b-b6f7-ae273411e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import urllib.request  # for downloading videos\n",
    "import traceback\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import imutils\n",
    "import random\n",
    "import ffmpeg\n",
    "import keyboard  # using module keyboard\n",
    "import time\n",
    "import csv\n",
    "\n",
    "\n",
    "MAIN_WIDTH = 640  # 256, iphone(480), \n",
    "MAIN_HEIGHT = 480  # 192, iphone(320), \n",
    "MAX_NUM_OF_XY_KEYPOINTS_LIST = 46\n",
    "\n",
    "# utility function to quickly create non-existing function:\n",
    "def mkdir_if_none(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    \n",
    "# if something errored out, return pre-defined array:\n",
    "BACKUP_ARRAY = np.array([2.0, 2.0] * MAX_NUM_OF_XY_KEYPOINTS_LIST)\n",
    "\n",
    "\n",
    "# resize the frame to specified size\n",
    "def resize_with_padding(img, expected_size):\n",
    "    delta_width = expected_size[0] - img.size[0]\n",
    "    delta_height = expected_size[1] - img.size[1]\n",
    "    pad_width = delta_width // 2\n",
    "    pad_height = delta_height // 2\n",
    "    padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
    "    return ImageOps.expand(img, padding)\n",
    "\n",
    "\n",
    "\n",
    "# just get keypoints from given frame:\n",
    "def get_keypoints_from_frame(frame, holistic, mp_holistic, hand_keypoint_default=21, verbose=True):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        # if it came from OpenCV, we need to switch channel order:\n",
    "        frame = imutils.resize(frame, width=MAIN_WIDTH)\n",
    "        pil = Image.fromarray(frame)\n",
    "        frame = cv2.cvtColor(np.array(resize_with_padding(pil, (MAIN_WIDTH, MAIN_HEIGHT))), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        # make detections:\n",
    "        results = holistic.process(frame)\n",
    "        \n",
    "        # array for collecting frame coordinates from hands, elbows, and shoulders [PER GIVEN SINGLE FRAME]:\n",
    "        frame_coordinates = []\n",
    "            \n",
    "        ################################################################################\n",
    "        # get coordinates:\n",
    "        # hand points:\n",
    "        # RIGHT:\n",
    "        if results.right_hand_landmarks:\n",
    "            for r_h in results.right_hand_landmarks.landmark:\n",
    "                frame_coordinates.append(r_h.x)\n",
    "                frame_coordinates.append(r_h.y)\n",
    "        # if not detected, fill in out of bounds coordinates -> think it will help to have consistent error model to know that we can have that.\n",
    "        else:\n",
    "            for r_h in range(21):\n",
    "                frame_coordinates.append(2.0)\n",
    "                frame_coordinates.append(2.0)\n",
    "\n",
    "        # LEFT:\n",
    "        if results.left_hand_landmarks:\n",
    "            for l_h in results.left_hand_landmarks.landmark:\n",
    "                frame_coordinates.append(l_h.x)\n",
    "                frame_coordinates.append(l_h.y)\n",
    "        else:\n",
    "            for l_h in range(21):\n",
    "                frame_coordinates.append(2.0)\n",
    "                frame_coordinates.append(2.0)\n",
    "\n",
    "        # SHOULDERS AND ELBOWS:\n",
    "        if results.pose_landmarks:\n",
    "            # top torse keypoints:\n",
    "            r_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_ELBOW]\n",
    "            frame_coordinates.append(r_elbow.x)\n",
    "            frame_coordinates.append(r_elbow.y)\n",
    "\n",
    "            l_elbow = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_ELBOW]\n",
    "            frame_coordinates.append(l_elbow.x)\n",
    "            frame_coordinates.append(l_elbow.y)\n",
    "\n",
    "            r_shoulder = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.RIGHT_SHOULDER]\n",
    "            frame_coordinates.append(r_shoulder.x)\n",
    "            frame_coordinates.append(r_shoulder.y)\n",
    "\n",
    "            l_shoulder = results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_SHOULDER]\n",
    "            frame_coordinates.append(l_shoulder.x)\n",
    "            frame_coordinates.append(l_shoulder.y)\n",
    "        else:\n",
    "            frame_coordinates.append(2.0)  # fake x axis for r_elbow\n",
    "            frame_coordinates.append(2.0)  # fake y axis for r_elbow\n",
    "            \n",
    "            frame_coordinates.append(2.0)  # fake x axis for l_elbow\n",
    "            frame_coordinates.append(2.0)  # fake y axis for l_elbow\n",
    "            \n",
    "            frame_coordinates.append(2.0)  # fake x axis for r_shoulder\n",
    "            frame_coordinates.append(2.0)  # fake y axis for r_shoulder\n",
    "            \n",
    "            frame_coordinates.append(2.0)  # fake x axis for l_shoulder\n",
    "            frame_coordinates.append(2.0)  # fake y axis for l_shoulder\n",
    "\n",
    "        if verbose:\n",
    "            for coord_idx in range(0, len(frame_coordinates), 2):\n",
    "                lm = frame_coordinates[coord_idx], frame_coordinates[coord_idx+1]\n",
    "                cx, cy = int(lm[0]*frame.shape[1]), int(lm[1]*frame.shape[0])\n",
    "                cv2.circle(frame, (cx, cy), 2, (255, 0, 0), cv2.FILLED)\n",
    "        ################################################################################\n",
    "        # return np.array(frame_coordinates), frame\n",
    "        return frame_coordinates, frame\n",
    "    except:\n",
    "        print(\"<<<ERROR IN GETTING KEYPOINTS>>>\")\n",
    "        print(traceback.format_exc())\n",
    "        print(\"---\")\n",
    "        return BACKUP_ARRAY, frame\n",
    "    \n",
    "\n",
    "\n",
    "# run video and process videos through media pipe:\n",
    "def process_video(holistic, mp_holistic, keypoints_data_dir, action_w_source_lst, num_of_sequences_per_class, sequence_length, fps=30):\n",
    "    \n",
    "    # mkdir for main keypoints directory:\n",
    "    mkdir_if_none(keypoints_data_dir)\n",
    "    \n",
    "    print(f\"Action order: {[i[0] for i in action_w_source_lst]}\")\n",
    "    \n",
    "    # go over each action (class):\n",
    "    for action, video_source in action_w_source_lst:\n",
    "        \n",
    "        try:\n",
    "            # initiate class, if no such exists:\n",
    "            class_dir_path = os.path.join(keypoints_data_dir, action)\n",
    "            mkdir_if_none(class_dir_path)\n",
    "\n",
    "            want_to_quit_action = False\n",
    "\n",
    "            #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "            video_source = video_source\n",
    "            cap = cv2.VideoCapture(video_source)\n",
    "            cap.set(cv2.CAP_PROP_FPS, fps)\n",
    "\n",
    "            # Check if the video is opened correctly\n",
    "            if not cap.isOpened():\n",
    "                raise IOError(f\"Cannot open video {video_source}\")\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                # return np.array([])\n",
    "                print(f\"<<<WARNING: Failed to open video source = {video_source}\")\n",
    "\n",
    "            print(f\"Processing video: {video_source} with {fps}FPS\")\n",
    "            time.sleep(1)\n",
    "            #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "            # keep count of sample per class:\n",
    "            sequence_counter = 0\n",
    "            # for each sequence, keep storing it until getting them all (30 recs per each):\n",
    "            for sequence in range(num_of_sequences_per_class):\n",
    "                # get sequence array, to get them all in single place:\n",
    "                frame_coordinates_stack = []\n",
    "                save_file_as = os.path.join(class_dir_path, f\"{sequence_counter}.csv\")  # npy\")\n",
    "\n",
    "                if os.path.exists(save_file_as):\n",
    "                    print(f\"Such file ({save_file_as}) already exist. Skipping to next sample...\")\n",
    "                    sequence_counter+=1\n",
    "                    continue\n",
    "\n",
    "                #################################################################\n",
    "                # go over each frame and get it's associated keypoints:\n",
    "                for frame_num in range(sequence_length):  # total range per video (30FPS per second is default)\n",
    "\n",
    "                    try:\n",
    "                        ret, frame = cap.read()\n",
    "                        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                        if ret:\n",
    "                            retrieved_frame_keypoints, frame = get_keypoints_from_frame(frame, holistic, mp_holistic)\n",
    "                            # print(f\"len(retrieved_frame_keypoints): {len(retrieved_frame_keypoints)}\")\n",
    "                            frame_coordinates_stack.append(retrieved_frame_keypoints)\n",
    "\n",
    "                            cv2.imwrite(os.path.join(os.getcwd(), \"tmp\", f\"{action}_{frame_num}.jpg\"), frame)\n",
    "\n",
    "                            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                            if frame_num == 0:\n",
    "                                cv2.putText(frame, f\"STARTING COLLECTION for {action}\", (120,200),\n",
    "                                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 4, cv2.LINE_AA)\n",
    "                                cv2.putText(frame, f\"Collecting frames for {action} Video Number {sequence}/{num_of_sequences_per_class}\", (15,22),\n",
    "                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "                                cv2.imshow('Raw Video Feed', frame)\n",
    "                                cv2.waitKey(2000)\n",
    "                            else:\n",
    "                                cv2.putText(frame, f\"Collecting frames for {action} Video Number {sequence}/{num_of_sequences_per_class}\", (15,12),\n",
    "                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "                            cv2.imshow('Raw Video Feed', frame)\n",
    "                            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "                        else:\n",
    "                            print(f\"<<<WARNING: Frame was skipped...>>>\")\n",
    "\n",
    "                        try:\n",
    "                            pressedKey = cv2.waitKey(1) & 0xFF\n",
    "                            if pressedKey == ord('w'):\n",
    "                                print(\"Requested to pause. Waiting until 'Esc' is pressed\")\n",
    "                                try:\n",
    "                                    keyboard.wait('esc')\n",
    "                                    print(\"'Esc' was pressed. Continuing...\")\n",
    "                                    time.sleep(1)\n",
    "                                except:\n",
    "                                    pass\n",
    "                            elif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                                print(\"'q' was pressed. Ending connection with current sequence.\")\n",
    "                                want_to_quit_action = True\n",
    "                                break\n",
    "                        except:\n",
    "                            break\n",
    "                    except:\n",
    "                        print(\"\\nFailed in cap.read(). Read description:\")\n",
    "                        print(traceback.format_exc())\n",
    "                        print(\"Returning empty stack\")\n",
    "                        cap.release()\n",
    "                        cv2.destroyAllWindows()\n",
    "                        print(\"---\")\n",
    "                        # continue\n",
    "                        break\n",
    "\n",
    "                #################################################################\n",
    "                #'''\n",
    "\n",
    "                if want_to_quit_action:\n",
    "                    break\n",
    "\n",
    "                if len(frame_coordinates_stack) >= sequence_length:\n",
    "                    # np.savetxt(save_file_as, np.array(frame_coordinates_stack[:sequence_length]), delimiter=\",\")\n",
    "                    # np.save(save_file_as, np.array(frame_coordinates_stack[:sequence_length])) # save\n",
    "                    with open(save_file_as, \"w\", newline=\"\\n\") as f:\n",
    "                        writer = csv.writer(f)\n",
    "                        writer.writerows(frame_coordinates_stack[:sequence_length])\n",
    "                    sequence_counter+=1\n",
    "                else:\n",
    "                    print(f\"<<<WARNING: Gathered less coordinates than expected: {sequence_counter}/{sequence_length}>>>\")\n",
    "        except:\n",
    "            print(\"======================\")\n",
    "            print(\"Stop triggered:\")\n",
    "            print(traceback.format_exc())\n",
    "            print(\"======================\")\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            cap.release()\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            cv2.destroyAllWindows()\n",
    "        except:\n",
    "            pass\n",
    "        continue\n",
    "        \n",
    "        print(\"Hit Enter to Continue to next Action\")\n",
    "        try:\n",
    "            keyboard.wait('enter')\n",
    "            print(\"'Enter' was pressed. Starting with next action in 3 seconds...\")\n",
    "            time.sleep(3)\n",
    "        except:\n",
    "            print(\"Pressed on Stop button. Exiting completely.\")\n",
    "            return\n",
    "        \n",
    "        \n",
    "def collect_keypoints_data():\n",
    "                    \n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \n",
    "    # Initialization:\n",
    "    # setup keypoint dataset directory to save:\n",
    "    dataset_dir = os.path.join(os.getcwd(), \"data\")\n",
    "    mkdir_if_none(dataset_dir)    \n",
    "    \n",
    "    # setup MediaPipe:\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    print(f\"mp.solutions: {mp.solutions}\")\n",
    "    holistic_model = mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5, static_image_mode=False)    \n",
    "    # End of Initiliazation\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "                    \n",
    "    keypoints_data_dir = os.path.join(os.getcwd(), \"keypoints_data\")\n",
    "                    \n",
    "    action_w_source_lst = [[\"hello\", 0], [\"world\", 0], [\"my\", 0], [\"me\", 0],\n",
    "                           [\"every\", 0], [\"moment\", 0], [\"is\", 0], [\"new\", 0],\n",
    "                           [\"beginning\", 0]]\n",
    "                    \n",
    "    num_of_sequences_per_class = 300  # for train: 60*5  # 60seconds * 5 => 5 minutes => 300 sequences\n",
    "    sequence_length = 30                    \n",
    "    \n",
    "    keypoints_detected_np = process_video(holistic_model, mp_holistic, keypoints_data_dir, action_w_source_lst, num_of_sequences_per_class, sequence_length, fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b82960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mp.solutions: <module 'mediapipe.python.solutions' from 'C:\\\\Users\\\\lrspr\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\mediapipe\\\\python\\\\solutions\\\\__init__.py'>\n",
      "Action order: ['hello', 'world', 'my', 'me', 'every', 'moment', 'is', 'new', 'beginning']\n",
      "Processing video: 0 with 30FPS\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\hello\\0.csv) already exist. Skipping to next sample...\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\hello\\1.csv) already exist. Skipping to next sample...\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\hello\\2.csv) already exist. Skipping to next sample...\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\hello\\3.csv) already exist. Skipping to next sample...\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\hello\\4.csv) already exist. Skipping to next sample...\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\hello\\5.csv) already exist. Skipping to next sample...\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\hello\\6.csv) already exist. Skipping to next sample...\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\hello\\7.csv) already exist. Skipping to next sample...\n",
      "'q' was pressed. Ending connection with current sequence.\n",
      "Processing video: 0 with 30FPS\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\world\\0.csv) already exist. Skipping to next sample...\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\world\\1.csv) already exist. Skipping to next sample...\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\world\\2.csv) already exist. Skipping to next sample...\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\world\\3.csv) already exist. Skipping to next sample...\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\world\\4.csv) already exist. Skipping to next sample...\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\world\\5.csv) already exist. Skipping to next sample...\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\world\\6.csv) already exist. Skipping to next sample...\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\world\\7.csv) already exist. Skipping to next sample...\n",
      "Such file (C:\\Users\\lrspr\\Desktop\\Masters_Program\\690_Deep_Learning\\Projects\\Project_3_due_12_05_22\\keypoints_data\\world\\8.csv) already exist. Skipping to next sample...\n",
      "Requested to pause. Waiting until 'Esc' is pressed\n",
      "'Esc' was pressed. Continuing...\n",
      "Requested to pause. Waiting until 'Esc' is pressed\n",
      "'Esc' was pressed. Continuing...\n",
      "'q' was pressed. Ending connection with current sequence.\n",
      "======================\n",
      "Stop triggered:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\lrspr\\AppData\\Local\\Temp\\ipykernel_6212\\4169191177.py\", line 149, in process_video\n",
      "    cap = cv2.VideoCapture(video_source)\n",
      "KeyboardInterrupt\n",
      "\n",
      "======================\n"
     ]
    }
   ],
   "source": [
    "collect_keypoints_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cf47554-64dd-4646-bad9-70b1f7f4dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "\n",
    "# size validation check, for collection verifications, to check if all rows contained (x,y) values for all 46 points:\n",
    "def check_sizes(mode):\n",
    "    data_dir = f\"./keypoints_data/{mode}\"\n",
    "    for f in os.listdir(data_dir):\n",
    "        f_path = os.path.join(data_dir, f)\n",
    "        for i in os.listdir(f_path):\n",
    "            file_name = os.path.join(f_path, i)\n",
    "            tmp_file = genfromtxt(file_name, delimiter=',')\n",
    "            if tmp_file.shape != (30, 92):\n",
    "                print(\"{tmp_file} has different shape: {tmp_file.shape}\")\n",
    "            \n",
    "check_sizes(mode=\"train\")\n",
    "check_sizes(mode=\"val\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
